{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e26fe7",
   "metadata": {},
   "source": [
    "# Example for 1 billion molecules clustered from the Enamine Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314c3994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import rdBase\n",
    "blocker = rdBase.BlockLogs() # Supress RDKIT Warnings\n",
    "from spiq.utils import format_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a8df4",
   "metadata": {},
   "source": [
    "# Getting Training data\n",
    "Enamine Database can be downloaded from `https://enamine.net/compound-collections/real-compounds/real-database`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spiq import DataStreamer\n",
    "from spiq import FingerprintCalculator\n",
    "from tqdm import tqdm\n",
    "from spiq.utils import save_chunk\n",
    "\n",
    "# These can be useful to make the transformation from .txt/sdf format to MQN fingerprint easier\n",
    "stream = DataStreamer()  \n",
    "fp_calc = FingerprintCalculator()\n",
    "\n",
    "chunksize = 10_000_000\n",
    "path ='~/Downloads/EnamineFiles/1B_Enamine_Molecules.txt'  # Replace with the actual path to the file\n",
    "smiles_generator = stream.parse_input(\n",
    "     path,\n",
    "     chunksize=chunksize, # chunk size, whatever fits comfortably in RAM. This process is paralellized \n",
    "     smiles_col=1 # column where the SMILES strings are located\n",
    "     ) \n",
    "\n",
    "# For 1 billion molecules, around 100M should be enough to train the encoder and cluster algorithm\n",
    "iterations = int(100_000_000 / chunksize)\n",
    "for i, chunk in tqdm(enumerate(smiles_generator), desc='Calculating Fingerprints', total=iterations):\n",
    "    chunk_fp = fp_calc.FingerprintFromSmiles(chunk, fp=\"mqn\") # Calculate MQN fingerprint\n",
    "    output_path = 'tmp/' # Define your output path, make sure enough space is available\n",
    "    save_chunk(chunk_fp,\n",
    "               name='enamine-chunk',\n",
    "               chunk_index=i,  # Each file will be named name+chunk_index.npy\n",
    "               output_dir=output_path)\n",
    "\n",
    "    if i == iterations:\n",
    "       break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8effda3e",
   "metadata": {},
   "source": [
    " At this point we've only transformed our SMILES strings into its corresponding 42 dimensional MQN fingerprints. Now is time to transform those MQN fingerprints into PQ codes.\n",
    "\n",
    "# Product Quantization\n",
    "\n",
    "We fit the PQ encoder using our training data. My enamine data is already randomly mixed. If use the file directly as downlaoded from the enamine website make sure to properly select the chunk for training since those files come sorted by size\n",
    "\n",
    "- `K`refers to the number of centroids to be used when running KMeans on each subvector. \n",
    "\n",
    "- `m`is the number of subvectors (splits) from our input data. \n",
    " \n",
    "- `iterations`is the maximum number of iterations each KMeans is going to do. \n",
    "\n",
    "With higher `K`and `iterations`, higher training times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67096f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10000000, 42), MB: 840.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "chunk = np.load('/mnt/samsung_2tb/tmp/enamine-chunk_00000.npy')\n",
    "print(f\"Shape: {(chunk.shape)}, MB: {chunk.nbytes/1e6}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda62f7",
   "metadata": {},
   "source": [
    "This is what a normal chunk of 10M MQN fingerprints take in space. In order to train the encoder that will transform MQN -> PQ codes, we can use 1 chunk since it will comfortably fit in memory. Feel free to load multiple chunks although 10M should be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3108cd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PQ-codes: 100%|██████████| 6/6 [15:30<00:00, 155.03s/it]\n"
     ]
    }
   ],
   "source": [
    "from spiq import PQEncoder\n",
    "pq_encoder = PQEncoder(k=256, m=6, iterations=10)\n",
    "pq_encoder.fit(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868c661",
   "metadata": {},
   "source": [
    "After training we can save/load the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef22b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(pq_encoder, 'enamine_pqencoder_tutorial.joblib')\n",
    "\n",
    "# To load it\n",
    "pq_encoder = joblib.load('enamine_pqencoder_tutorial.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829bfb82",
   "metadata": {},
   "source": [
    "We can check some atributes: \n",
    "\n",
    "- `.codebook_cluster_centers` are the centroids coordinates gathered from each KMeans run on every subvector. Since we have 4 splits, 256 centroids and the subvectors are of size 1024/4 = 256, then the codebook is shape (4, 256, 256)\n",
    "\n",
    "After the `pq_encoder` is trained, the encoder has an attribute to account for the training process. If we try to use transform without fitting we would get an Error. So know, we check that the ecoder was in fact trained. \n",
    "\n",
    "If we want to access all the `KMeans`attributes that one would normally get from sklearn, we can do so using `pq_trained` and use any attribute you would normally use. Like `.labels_` to check the index of the centroids for each training sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b8a8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the codebook is:  (6, 256, 7)\n",
      "Is the encoder trained?  True\n",
      "The total number of lables [164 183 183 ...   9  20  86]  is: 10000000\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of the codebook is: \", pq_encoder.codewords.shape)\n",
    "print(\"Is the encoder trained? \", pq_encoder.encoder_is_trained)\n",
    "print(f\"The total number of lables {pq_encoder.pq_trained[0].labels_}  is: {len(pq_encoder.pq_trained[0].labels_)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce878c3",
   "metadata": {},
   "source": [
    "After the training process we can create our PQ codes.\n",
    "The PQCodes are going to be of shape `(Number of samples, m)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "763f58a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating PQ-codes: 100%|██████████| 6/6 [00:03<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10000000, 6), MB: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pq_codes = pq_encoder.transform(chunk)\n",
    "print(f\"Shape: {pq_codes.shape}, MB: {pq_codes.nbytes/1e6}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b3c07",
   "metadata": {},
   "source": [
    "# Inverse Transformation\n",
    "We can also transform PQ codes to the original MQN vector and see how lossly the process is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8af696e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23  0  0  0  0  0  0  1  4  1  0 29 13  1  0  9  8  0  8  7  6  1  1  0\n",
      "  0  5  4  3  0 11  6  0  0  0  1  2  0  0  0  0  0  0] (1000000, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 15.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3 140 163  49  11   1]\n",
      "[23  0  0  0  0  0  0  1  4  1  0 29 13  0  0  8  8  0  8  8  5  1  1  0\n",
      "  0  4  3  3  0 11  6  0  0  0  1  2  0  0  0  0  0  0] (1000000, 42)\n",
      "Reconversion process:\n",
      "Reconstruction fidelity: 92.02%\n"
     ]
    }
   ],
   "source": [
    "# Load a different chunk\n",
    "mqn_chunk_2 = np.load('/mnt/samsung_2tb/tmp/enamine-chunk_00001.npy')\n",
    "\n",
    "# Now that we trained the encoder we can convert MQN into PQCode and also test how lossy our compressio method is comparing the original MQN matrix vs the reconstructed one\n",
    "\n",
    "ogMQN = mqn_chunk_2[:1_000_000]\n",
    "print(ogMQN[:1].reshape(42,), ogMQN.shape)\n",
    "\n",
    "pqcode = pq_encoder.transform(ogMQN)\n",
    "print(pqcode[:1].reshape(6, ))\n",
    "\n",
    "rMQN = pq_encoder.inverse_transform(pqcode)\n",
    "print(rMQN[:1].reshape(42,), rMQN.shape)\n",
    "\n",
    "print('Reconversion process:')\n",
    "\n",
    "# MSE / RMSE\n",
    "mse  = np.mean((ogMQN - rMQN)**2)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "def fidelity_frobenius(X, X_hat):\n",
    "    num = np.linalg.norm(X - X_hat, 'fro')\n",
    "    den = np.linalg.norm(X, 'fro')\n",
    "    return (1 - num/den) * 100\n",
    "\n",
    "score = fidelity_frobenius(ogMQN, rMQN)\n",
    "print(f\"Reconstruction fidelity: {score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef3244",
   "metadata": {},
   "source": [
    "# Cluster\n",
    "\n",
    "Now to the clustering part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462074b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spiq import PQKMeans\n",
    "import time\n",
    "\n",
    "kmeans = PQKMeans(encoder=pq_encoder, k=10_000, iteration=20)\n",
    "s = time.time()\n",
    "kmeans.fit(pq_codes)\n",
    "e = time.time()\n",
    "\n",
    "print(f'Training with {len(pq_codes)} molecules took: {format_time(e-s)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b01d9",
   "metadata": {},
   "source": [
    "We can of course save/load the Kmeans as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b524570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(kmeans, 'kmeans_trained')\n",
    "kmeans = joblib.load('kmeans_trained')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b3163",
   "metadata": {},
   "source": [
    "Finally, we can now get the cluster for every PQ code we pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e9277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49161, 13845, 59452, 70857, 92866, 53390, 86229, 76216, 10800,\n",
       "       35907, 28614, 36577, 37133,  7373, 78079, 86816, 32896, 54950,\n",
       "       50991, 61871, 41623, 62782, 13150, 18079, 17881, 90621, 16919,\n",
       "       89129, 41955, 51549, 28733,  6753,  5147, 10976, 94913, 24994,\n",
       "       41253, 39846, 23494, 39025, 92625, 32737, 32737, 92866, 10623,\n",
       "       66455, 41056, 32782, 60229, 86723, 70828, 25645, 19690, 66414,\n",
       "       57948, 62575, 37133, 79107, 80172, 25645, 42460, 87881, 64548,\n",
       "       84159, 22125, 75078, 86370, 85735, 45628, 55806, 71367, 31446,\n",
       "        7686,  2500, 65864, 95833, 19290, 87959, 84159, 86816, 90085,\n",
       "       60947, 17623, 66380, 68923, 12279, 32309, 55541, 30332, 76162,\n",
       "       94913, 18079, 79672, 24077, 50207, 88405, 33924, 89928, 25457,\n",
       "       92866])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.predict(pq_codes[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00100a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 13.59it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 14.19it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 13.64it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 14.00it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 13.99it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 13.34it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 14.17it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 14.37it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 14.33it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 13.97it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 14.26it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 14.07it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 14.15it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 13.54it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 13.23it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 13.58it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 14.01it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 13.85it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 14.38it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 14.25it/s]\n",
      "Generating PQ-codes: 100%|██████████| 6/6 [00:00<00:00, 14.50it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cluster the file with the SMILES strings\n",
    "import os\n",
    "import pandas as pd \n",
    "from spiq import DataStreamer, FingerprintCalculator\n",
    "from tqdm import tqdm\n",
    "\n",
    "stream = DataStreamer()  \n",
    "fp_calc = FingerprintCalculator()\n",
    "\n",
    "output_path = '/mnt/samsung_2tb/tmp/'\n",
    "for i, chunk in enumerate(stream.parse_input('/mnt/10tb_hdd/cleaned_enamine_10b/output_file_0.cxsmiles', chunksize=1_000_000, smiles_col=1)):\n",
    "    chunk_pq_code = pq_encoder.transform(fp_calc.FingerprintFromSmiles(chunk, 'mqn'))\n",
    "    labels = kmeans.predict(chunk_pq_code)\n",
    "    df = pd.DataFrame({\n",
    "        'smiles': chunk, \n",
    "        'cluster_id': labels\n",
    "    })\n",
    "    df.to_parquet(os.path.join(output_path, f'clustered_chunk_{i}.parquet'), index=False)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spiq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
