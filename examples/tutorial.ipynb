{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spiq.streamer.data_streamer import DataStreamer\n",
    "from spiq.utils.fingerprints import FingerprintCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINGERPRINT MODULE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple example on how the API works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fingerprints shape: (4, 2048)\n",
      "Fingerprint 1 [0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Define a list of SMILES strings\n",
    "smiles_list = [\"CCO\", \"C1CCCCC1\", \"O=C=O\", \"O=C=O\"]\n",
    "\n",
    "# Define fingerprint parameters\n",
    "params = {'fpSize': 2048, 'radius': 2}\n",
    "\n",
    "# Create an instance of FingerprintCalculator\n",
    "calculator = FingerprintCalculator()\n",
    "\n",
    "# Compute fingerprints for the list of SMILES strings\n",
    "fingerprints = calculator.FingerprintFromSmiles(smiles_list, 'morgan', **params)\n",
    "\n",
    "# Display the shape of the output fingerprint array\n",
    "print(f\"Fingerprints shape: {fingerprints.shape}\")\n",
    "print(\"Fingerprint 1\", fingerprints[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use the API for loading fingerprints in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      " Fingerprints calculated: 10,000"
     ]
    }
   ],
   "source": [
    "# Import iterator method\n",
    "ds = DataStreamer()\n",
    "\n",
    "chunksize = 1_230\n",
    "smiles= ds.parse_input(input_path='../data/data_lite.txt', chunksize=chunksize)\n",
    "print(type(smiles)) # This is only the generator, in order to get each chunk of data we need to iterate\n",
    "\n",
    "count = 0\n",
    "for smiles_chunk in smiles:\n",
    "     count += len(smiles_chunk)\n",
    "     calculator.FingerprintFromSmiles(smiles_chunk, 'morgan', **params)\n",
    "     print(f\"\\r Fingerprints calculated: {count:,}\", end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to save each chunk as a separate file -ideal for large chunks that we could use later- then `save_chunk` from the `helper_functions.py`is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fingerprints calculated: 10,000"
     ]
    }
   ],
   "source": [
    "from spiq.utils.helper_functions import save_chunk\n",
    "\n",
    "smiles= ds.parse_input(input_path='../data/data_lite.txt', chunksize=chunksize)\n",
    "\n",
    "count = 0\n",
    "for idx, smiles_chunk in enumerate(smiles):\n",
    "    count += len(smiles_chunk)\n",
    "    fp_chunk = calculator.FingerprintFromSmiles(smiles_chunk, 'morgan', **params)\n",
    "    save_chunk(fp_chunk, output_dir='../data/', chunk_index=idx, file_format='npy')\n",
    "    print(f\"\\r Fingerprints calculated: {count:,}\", end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 1024) 10241024\n"
     ]
    }
   ],
   "source": [
    "#First we load calcualte our fingerprints \n",
    "with open('../data/data_lite.txt', 'r') as file:\n",
    "    smiles = file.read().split('\\n')\n",
    "fingerprints = calculator.FingerprintFromSmiles(smiles, 'morgan', fpSize=1024, radius=3)\n",
    "\n",
    "print(fingerprints.shape, fingerprints.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit the PQ encoder using our training data. \n",
    "`K`refers to the number of centroids to be used when running KMeans on each subvector. \n",
    "`m`is the number of subvectors (splits) from our input data. \n",
    "`iterations`is the maximum number of iterations each KMeans is going to do. \n",
    "With higher `K`and `iterations`, higher training times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PQEncoder: 100%|██████████| 4/4 [00:01<00:00,  3.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from spiq.encoder.encoder import PQEncoder\n",
    "\n",
    "pq_encoder = PQEncoder(k=256, m=4, iterations=10)\n",
    "pq_encoder.fit(fingerprints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check some atributes: \n",
    "`.codebook_cluster_centers` are the centroids coordinates gathered from each KMeans run on every subvector. Since we have 4 splits, 256 centroids and the subvectors are of size 1024/4 = 256, then the codebook is shape (4, 256, 256)\n",
    "After the `pq_encoder` is fitted, the encoder has an attribute to account for the training process. If we try to use transform without fitting we would get an Error. So know, we check that the ecoder was in fact trained. \n",
    "If we want to access all the `KMeans`attributes that one would normally get from sklearn, we can do so using the attribute `pq_trained` and use any attribute you would normally use. Like `.labels_` to check the index of the centroids for each training sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the codebook is:  (4, 256, 256)\n",
      "Is the encoder trained?  True\n",
      "The lables: [212 212 212 ... 153  60  53] are of length: 10001\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of the codebook is: \", pq_encoder.codebook_cluster_centers.shape)\n",
    "print(\"Is the encoder trained? \", pq_encoder.encoder_is_trained)\n",
    "print(f\"The lables: {pq_encoder.pq_trained.labels_} are of length: {len(pq_encoder.pq_trained.labels_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training process we can create our PQ codes.\n",
    "The PQCodes are going to be of shape `(Number of samples, m)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,000,001 fingerprints of 1024 dimensions to be transformed into PQ-codes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating PQ-codes: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming 1,000,001 fingeprints took 4.77 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import time\n",
    "\n",
    "#First we load and calculate our fingerprints \n",
    "with open('../data/training_data.txt', 'r') as file:\n",
    "    smiles = file.read().split('\\n')\n",
    "X_test= calculator.FingerprintFromSmiles(smiles, 'morgan', fpSize=1024, radius=3)\n",
    "\n",
    "print(f\"{X_test.shape[0]:,} fingerprints of {X_test.shape[1]} dimensions to be transformed into PQ-codes\")\n",
    "\n",
    "s = time.time()\n",
    "X_pq_code = pq_encoder.transform(X_test)\n",
    "e = time.time()\n",
    "print(f\"Transforming {X_test.shape[0]:,} fingeprints took {(e-s):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since we have a PQ-code of 4 digits and each digit can take value {0,255} then the number of theoretical unique PQ-codes we can get is therefore $256^4 = 4,294,967,296$. However we can test that this is much less in reality.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 53,  53,  53, 149],\n",
       "       [ 53,  53,  19, 125],\n",
       "       [ 53,  53,  53,  21],\n",
       "       ...,\n",
       "       [242,  53, 195, 212],\n",
       "       [ 53,  86, 243, 172],\n",
       "       [ 53,  53,  53,  53]], shape=(1000001, 4), dtype=uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pq_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique 4-dim vectors: 89514\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Count unique rows\n",
    "unique_rows = np.unique(X_pq_code, axis=0)\n",
    "num_unique_vectors = unique_rows.shape[0]\n",
    "print(\"Number of unique 4-dim vectors:\", num_unique_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of transforming the binary fingerprints into PQ-codes is that we are storing (almost) the same information in a much more efficient way. We can check that the amount of memory required to store the same data is 256x times less\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input of shape: (1000001, 1024) and size of 1,024,001,024 bytes is now transformed into shape (1000001, 4) and size of 4,000,004 bytes\n",
      "This is 256 times more memory efficient\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original input of shape: {X_test.shape} and size of {X_test.nbytes:,} bytes is now transformed into shape {X_pq_code.shape} and size of {X_pq_code.nbytes:,} bytes\")\n",
    "print(f\"This is {int(X_test.nbytes / X_pq_code.nbytes)} times more memory efficient\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spiq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
